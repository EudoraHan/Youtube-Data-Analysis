{"cells":[{"cell_type":"markdown","source":["### Youtube Comment Analysis\nIn this notebook, we have a dataset of user comments for youtube videos related to animals or pets. We will attempt to identify cat or dog owners based on these comments, find out the topics important to them, and then identify video creators with the most viewers that are cat or dog owners."],"metadata":{}},{"cell_type":"markdown","source":["#### 0. Data Exploration and Cleaning"],"metadata":{}},{"cell_type":"code","source":["# read data\ndf_raw = spark.read.load(\"/FileStore/tables/animals_comments.csv\", format='csv', header = True, inferSchema = True)\ndf_raw.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+------+--------------------+\n        creator_name|userid|             comment|\n+--------------------+------+--------------------+\n        Doug The Pug|  87.0|I shared this to ...|\n        Doug The Pug|  87.0|  Super cute  üòÄüêïüê∂|\n         bulletproof| 530.0|stop saying get e...|\n       Meu Zool√≥gico| 670.0|Tenho uma jiboia ...|\n              ojatro|1031.0|I wanna see what ...|\n     Tingle Triggers|1212.0|Well shit now Im ...|\nHope For Paws - O...|1806.0|when I saw the en...|\nHope For Paws - O...|2036.0|Holy crap. That i...|\n          Life Story|2637.0|Ê≠¶Âô®„ÅØ„ÇØ„Ç®„Çπ„Éà„ÅßË≤∞„Åà„Çã„Çì„Åò„ÇÉ„Å™„ÅÑ„Çì...|\n       Brian Barczyk|2698.0|Call the teddy Larry|\n+--------------------+------+--------------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["df, df_rest= df_raw.randomSplit([0.05, 0.95])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# find user with preference of dog and cat\n# note: please propose your own approach and rule to label data \n#cond = (df_clean[\"comment\"].like(\"%my dog%\") | df_clean[\"comment\"].like(\"%I have a dog%\")\\\n#        | df_clean[\"comment\"].like(\"%my cat%\") | df_clean[\"comment\"].like(\"%I have a cat%\"))\n\ncond = (df[\"comment\"].like(\"%my dog%\") | df[\"comment\"].like(\"%I have a dog%\") | df[\"comment\"].like(\"%my dogs%\") | df[\"comment\"].like(\"%I have dog%\")\n        | df[\"comment\"].like(\"%my cat%\") | df[\"comment\"].like(\"%my cats%\") | df[\"comment\"].like(\"%I have a cat%\") | df[\"comment\"].like(\"%I have cat%\") \n        | df[\"comment\"].like(\"%my puppy%\") | df[\"comment\"].like(\"%my puppies%\") | df[\"comment\"].like(\"%my kitty%\") | df[\"comment\"].like(\"%my kitties%\") \n        | df[\"comment\"].like(\"%I have a kitty%\") | df[\"comment\"].like(\"%I have kitties%\") | df[\"comment\"].like(\"%I have a puppy%\") | df[\"comment\"].like(\"%I have puppies%\"))\n\ndf_clean = df.withColumn('dog_cat',  cond)\n\n# find user do not have \ndf_clean = df_clean.withColumn('no_pet', ~df_clean[\"comment\"].like(\"%my%\") & ~df_clean[\"comment\"].like(\"%have%\")) \ndf_clean = df_clean.withColumn('label', col(\"dog_cat\").cast(IntegerType()).cast('double'))\n\ndf_clean.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+--------+--------------------+-------+------+-----+\ncreator_name|  userid|             comment|dog_cat|no_pet|label|\n+------------+--------+--------------------+-------+------+-----+\n        null| 13596.0|The last sentence...|  false|  true|  0.0|\n        null| 20666.0|You look like old...|  false|  true|  0.0|\n        null| 21461.0|bhai fsd ki jhang...|  false|  true|  0.0|\n        null| 50782.0|You watch JTV and...|  false|  true|  0.0|\n        null| 55762.0|Its so funny how ...|  false|  true|  0.0|\n        null| 63684.0|Do you know what ...|  false|  true|  0.0|\n        null| 63747.0|you sure know how...|  false|  true|  0.0|\n        null| 79376.0|I loove this new ...|  false|  true|  0.0|\n        null| 87712.0|            fuck you|  false|  true|  0.0|\n        null|108962.0|I cant wait for m...|  false|  true|  0.0|\n        null|114122.0|i like the new na...|  false|  true|  0.0|\n        null|114776.0|I really enjoy yo...|  false|  true|  0.0|\n        null|114776.0|OMG THIS REACTION...|  false|  true|  0.0|\n        null|122137.0|                 Wow|  false|  true|  0.0|\n        null|130826.0|Nice video always...|  false|  true|  0.0|\n        null|130826.0|    Nice videpüíêüåπüòò|  false|  true|  0.0|\n        null|141180.0|This was when I s...|  false|  true|  0.0|\n        null|141180.0|           delena ‚ù§‚ù§|  false|  true|  0.0|\n        null|153390.0|          dangerous!|  false|  true|  0.0|\n        null|157301.0|Eu falo portugu√™s...|  false|  true|  0.0|\n+------------+--------+--------------------+-------+------+-----+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["for colume in df_clean.columns:\n  df_clean=df_clean.filter(df_clean[colume].isNotNull())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["from pyspark.ml.feature import StopWordsRemover\n\n# Define a list of stop words or use default list\nremover = StopWordsRemover()\nstopwords = remover.getStopWords() \n\n# Display some of the stop words\nstopwords[:10]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">196</span><span class=\"ansired\">]: </span>[&apos;i&apos;, &apos;me&apos;, &apos;my&apos;, &apos;myself&apos;, &apos;we&apos;, &apos;our&apos;, &apos;ours&apos;, &apos;ourselves&apos;, &apos;you&apos;, &apos;your&apos;]\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["# data preprocessing \nfrom pyspark.ml.feature import RegexTokenizer\n\nregexTokenizer = RegexTokenizer(inputCol=\"comment\", outputCol=\"text\", pattern=\"\\\\W\")\ndf_clean= regexTokenizer.transform(df_clean)\n "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["\nremover.setInputCol(\"text\")\nremover.setOutputCol(\"vector_no_stopw\")\ndf_clean = remover.transform(df_clean)\ndf_clean.show(10)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+---------+--------------------+-------+------+-----+--------------------+--------------------+\n        creator_name|   userid|             comment|dog_cat|no_pet|label|                text|     vector_no_stopw|\n+--------------------+---------+--------------------+-------+------+-----+--------------------+--------------------+\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...| 412776.0|Keep going yall g...|  false|  true|  0.0|[keep, going, yal...|[keep, going, yal...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...| 890114.0|Who still watchs ...|  false|  true|  0.0|[who, still, watc...|[still, watchs, v...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...| 913684.0|          BIG GELATO|  false|  true|  0.0|       [big, gelato]|       [big, gelato]|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1000411.0|All Gas No Brake!...|  false|  true|  0.0|[all, gas, no, br...|        [gas, brake]|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1000411.0|                Dope|  false|  true|  0.0|              [dope]|              [dope]|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1049623.0|damn this beat go...|  false|  true|  0.0|[damn, this, beat...|[damn, beat, go, ...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1060242.0|All i heard was m...|  false|  true|  0.0|[all, i, heard, w...|[heard, mumbles, ...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1327825.0|fresh as fuck lik...|  false|  true|  0.0|[fresh, as, fuck,...|[fresh, fuck, lik...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1449078.0|Favorite song on ...|  false|  true|  0.0|[favorite, song, ...|[favorite, song, ...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1507593.0|               die 6|  false|  true|  0.0|            [die, 6]|            [die, 6]|\n+--------------------+---------+--------------------+-------+------+-----+--------------------+--------------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["#### 1. Build the classifier \nIn order to train a model against the comments, you can use RegexTokenizer to split each comment into a list of words and then use Word2Vec or other model to convert the list to a word vector. What Word2Vec does is to map each word to a unique fixed-size vector and then transform each document into a vector using the average of all words in the document."],"metadata":{}},{"cell_type":"code","source":["%sh /home/ubuntu/databricks/python/bin/pip install nltk"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Requirement already satisfied: nltk in /databricks/python3/lib/python3.5/site-packages (3.4)\nRequirement already satisfied: singledispatch in /databricks/python3/lib/python3.5/site-packages (from nltk) (3.4.0.3)\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from nltk) (1.10.0)\nYou are using pip version 10.0.1, however version 18.1 is available.\nYou should consider upgrading via the &apos;pip install --upgrade pip&apos; command.\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["from nltk.stem.porter import *\n\n# Instantiate stemmer object\nstemmer = PorterStemmer()\n\n# Create stemmer python function\ndef stem(in_vec):\n    out_vec = []\n    for t in in_vec:\n        t_stem = stemmer.stem(t)\n        if len(t_stem) > 2:\n            out_vec.append(t_stem)       \n    return out_vec\n\n# Create user defined function for stemming with return type Array<String>\nfrom pyspark.sql.types import *\nstemmer_udf = udf(lambda x: stem(x), ArrayType(StringType()))\n\n# Create new column with vectors containing the stemmed tokens \ndf_clean = df_clean.withColumn(\"vector_stemmed\", stemmer_udf(\"vector_no_stopw\"))\n\ndf_clean.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+---------+--------------------+-------+------+-----+--------------------+--------------------+--------------------+\n        creator_name|   userid|             comment|dog_cat|no_pet|label|                text|     vector_no_stopw|      vector_stemmed|\n+--------------------+---------+--------------------+-------+------+-----+--------------------+--------------------+--------------------+\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...| 412776.0|Keep going yall g...|  false|  true|  0.0|[keep, going, yal...|[keep, going, yal...|[keep, yall, got,...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...| 890114.0|Who still watchs ...|  false|  true|  0.0|[who, still, watc...|[still, watchs, v...|[still, watch, vi...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...| 913684.0|          BIG GELATO|  false|  true|  0.0|       [big, gelato]|       [big, gelato]|       [big, gelato]|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1000411.0|All Gas No Brake!...|  false|  true|  0.0|[all, gas, no, br...|        [gas, brake]|             [brake]|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1000411.0|                Dope|  false|  true|  0.0|              [dope]|              [dope]|              [dope]|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1049623.0|damn this beat go...|  false|  true|  0.0|[damn, this, beat...|[damn, beat, go, ...|[damn, beat, craz...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1060242.0|All i heard was m...|  false|  true|  0.0|[all, i, heard, w...|[heard, mumbles, ...| [heard, mumbl, lol]|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1327825.0|fresh as fuck lik...|  false|  true|  0.0|[fresh, as, fuck,...|[fresh, fuck, lik...|[fresh, fuck, lik...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1449078.0|Favorite song on ...|  false|  true|  0.0|[favorite, song, ...|[favorite, song, ...|[favorit, song, a...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1507593.0|               die 6|  false|  true|  0.0|            [die, 6]|            [die, 6]|               [die]|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1522521.0|I think I.L Will ...|  false|  true|  0.0|[i, think, i, l, ...|[think, l, stop, ...|[think, stop, fuc...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1575340.0|you GD YOU BD YOU...|  false|  true|  0.0|[you, gd, you, bd...|[gd, bd, stone, b...| [stone, best, part]|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1704212.0|sounding like 201...|  false|  true|  0.0|[sounding, like, ...|[sounding, like, ...|[sound, like, 201...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1813654.0|Hold up just pads...|  false|  true|  0.0|[hold, up, just, ...|[hold, pads, nusk...|[hold, pad, nuski...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1823504.0|Dope song. Walked...|  false|  true|  0.0|[dope, song, walk...|[dope, song, walk...|[dope, song, walk...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1950851.0|Billy: Excuse me ...|  false|  true|  0.0|[billy, excuse, m...|[billy, excuse, o...|[billi, excus, op...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|2284744.0|I also see you go...|  false|  true|  0.0|[i, also, see, yo...|[also, see, got, ...|[also, see, got, ...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|2307162.0|I allways like th...|  false|  true|  0.0|[i, allways, like...|[allways, like, i...|[allway, like, in...|\n  100 Bigfoot Nights| 671840.0|I dont have have ...|  false| false|  0.0|[i, dont, have, h...|[dont, time, nons...|[dont, time, nons...|\n        100% Madison|1204930.0|Nice vid bro! you...|  false| false|  0.0|[nice, vid, bro, ...|[nice, vid, bro, ...|[nice, vid, bro, ...|\n+--------------------+---------+--------------------+-------+------+-----+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF\nfrom pyspark.sql.functions import col,size,count,when,isnan\nfrom pyspark.sql import *\nfrom functools import reduce\n\ndf_clean.na.drop()\nhashingTF = HashingTF(inputCol=\"vector_stemmed\", outputCol=\"tf\", numFeatures=200)\nfeaturizedData = hashingTF.transform(df_clean)\nfeaturizedData.na.drop()\n\nfeaturizedData.withColumn('userid', col('userid').cast('float').cast(IntegerType()))\n\nfeaturizedData.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+---------+--------------------+-------+------+-----+--------------------+--------------------+--------------------+--------------------+\n        creator_name|   userid|             comment|dog_cat|no_pet|label|                text|     vector_no_stopw|      vector_stemmed|                  tf|\n+--------------------+---------+--------------------+-------+------+-----+--------------------+--------------------+--------------------+--------------------+\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...| 412776.0|Keep going yall g...|  false|  true|  0.0|[keep, going, yal...|[keep, going, yal...|[keep, yall, got,...|(200,[7,9,134,136...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...| 890114.0|Who still watchs ...|  false|  true|  0.0|[who, still, watc...|[still, watchs, v...|[still, watch, vi...|(200,[0,2,29,75,1...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...| 913684.0|          BIG GELATO|  false|  true|  0.0|       [big, gelato]|       [big, gelato]|       [big, gelato]|(200,[100,198],[1...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1000411.0|All Gas No Brake!...|  false|  true|  0.0|[all, gas, no, br...|        [gas, brake]|             [brake]|    (200,[18],[1.0])|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1000411.0|                Dope|  false|  true|  0.0|              [dope]|              [dope]|              [dope]|    (200,[85],[1.0])|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1049623.0|damn this beat go...|  false|  true|  0.0|[damn, this, beat...|[damn, beat, go, ...|[damn, beat, craz...|(200,[5,63,130,16...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1060242.0|All i heard was m...|  false|  true|  0.0|[all, i, heard, w...|[heard, mumbles, ...| [heard, mumbl, lol]|(200,[30,107,157]...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1327825.0|fresh as fuck lik...|  false|  true|  0.0|[fresh, as, fuck,...|[fresh, fuck, lik...|[fresh, fuck, lik...|(200,[78,130,192,...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1449078.0|Favorite song on ...|  false|  true|  0.0|[favorite, song, ...|[favorite, song, ...|[favorit, song, a...|(200,[28,71,150],...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1507593.0|               die 6|  false|  true|  0.0|            [die, 6]|            [die, 6]|               [die]|   (200,[129],[1.0])|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1522521.0|I think I.L Will ...|  false|  true|  0.0|[i, think, i, l, ...|[think, l, stop, ...|[think, stop, fuc...|(200,[3,15,18,45,...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1575340.0|you GD YOU BD YOU...|  false|  true|  0.0|[you, gd, you, bd...|[gd, bd, stone, b...| [stone, best, part]|(200,[35,140,163]...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1704212.0|sounding like 201...|  false|  true|  0.0|[sounding, like, ...|[sounding, like, ...|[sound, like, 201...|(200,[10,24,25,27...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1813654.0|Hold up just pads...|  false|  true|  0.0|[hold, up, just, ...|[hold, pads, nusk...|[hold, pad, nuski...|(200,[51,78,125,1...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1823504.0|Dope song. Walked...|  false|  true|  0.0|[dope, song, walk...|[dope, song, walk...|[dope, song, walk...|(200,[5,44,45,53,...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|1950851.0|Billy: Excuse me ...|  false|  true|  0.0|[billy, excuse, m...|[billy, excuse, o...|[billi, excus, op...|(200,[7,12,30,50,...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|2284744.0|I also see you go...|  false|  true|  0.0|[i, also, see, yo...|[also, see, got, ...|[also, see, got, ...|(200,[7,47,59,115...|\n#CameraLord‚Ñ¢ ‚Ä¢ Ko...|2307162.0|I allways like th...|  false|  true|  0.0|[i, allways, like...|[allways, like, i...|[allway, like, in...|(200,[39,82,130],...|\n  100 Bigfoot Nights| 671840.0|I dont have have ...|  false| false|  0.0|[i, dont, have, h...|[dont, time, nons...|[dont, time, nons...|(200,[105,135,157...|\n        100% Madison|1204930.0|Nice vid bro! you...|  false| false|  0.0|[nice, vid, bro, ...|[nice, vid, bro, ...|[nice, vid, bro, ...|(200,[32,54,67,68...|\n+--------------------+---------+--------------------+-------+------+-----+--------------------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["idf = IDF(inputCol=\"tf\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["rescaledData = idfModel.transform(featurizedData)\nrescaledData.select(\"label\", \"features\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+--------------------+\nlabel|            features|\n+-----+--------------------+\n  0.0|(200,[7,9,134,136...|\n  0.0|(200,[0,2,29,75,1...|\n  0.0|(200,[100,198],[4...|\n  0.0|(200,[18],[3.3287...|\n  0.0|(200,[85],[4.1888...|\n  0.0|(200,[5,63,130,16...|\n  0.0|(200,[30,107,157]...|\n  0.0|(200,[78,130,192,...|\n  0.0|(200,[28,71,150],...|\n  0.0|(200,[129],[3.503...|\n  0.0|(200,[3,15,18,45,...|\n  0.0|(200,[35,140,163]...|\n  0.0|(200,[10,24,25,27...|\n  0.0|(200,[51,78,125,1...|\n  0.0|(200,[5,44,45,53,...|\n  0.0|(200,[7,12,30,50,...|\n  0.0|(200,[7,47,59,115...|\n  0.0|(200,[39,82,130],...|\n  0.0|(200,[105,135,157...|\n  0.0|(200,[32,54,67,68...|\n+-----+--------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["pet = rescaledData.filter(\"label=1.0\")\npet_train, pet_test = pet.randomSplit([0.8, 0.2])\nnopet = rescaledData.filter(\"label=0.0\")\nsampleRatio = float(pet.count()) / float(nopet.count())\nsample_nopet = nopet.sample(False, sampleRatio)\ndf_sample = pet.unionAll(sample_nopet)\nsample_nopet_train, sample_nopet_test = sample_nopet.randomSplit([0.8, 0.2])\n\ndf_train = pet_train.unionAll(sample_nopet_train)\ndf_test = pet_test.unionAll(sample_nopet_test)\nprint ('training size',df_train.count())\nprint ('testing size',df_test.count())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">training size 3128\ntesting size 746\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n\nlr = LogisticRegression(maxIter=10,featuresCol='features', labelCol='label')\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(lr.regParam, [0.01,0.1]) \\\n    .build()\n\nevaluator=BinaryClassificationEvaluator()\ncrossval = CrossValidator(estimator = lr,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=5)\n\n\ncvModel = crossval.fit(df_train)\nbest_model = cvModel.bestModel\ntrainingSummary = best_model.summary"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["path = \"/FileStore/tables/\"\n\nbest_model.save(path + 'best_model')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["prediction_train = best_model.transform(df_train)\nprediction_test = best_model.transform(df_test)\naccuracy_train = prediction_train.filter(prediction_train.label == prediction_train.prediction).count()/float(df_train.count())\naccuracy_test = prediction_test.filter(prediction_test.label == prediction_test.prediction).count()/float(df_test.count())\n\nprint('Training set areaUnderROC: ' + str(evaluator.evaluate(prediction_train)))\nprint('Testing set areaUnderROC ' + str(evaluator.evaluate(prediction_test)))\nprint('Training set accuracy: ' + str(accuracy_train))\nprint('Testing set accuracy ' + str(accuracy_test))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Training set areaUnderROC: 0.973024096720583\nTesting set areaUnderROC 0.9551711002019709\nTraining set accuracy: 0.9386189258312021\nTesting set accuracy 0.9155495978552279\n</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["#### 2. Classify All The Users\nWe can now apply the cat/dog classifiers to all the other users in the dataset."],"metadata":{}},{"cell_type":"code","source":["prediction = best_model.transform(rescaledData)\n\ntotal_pet_owner = prediction.filter(\"prediction = 1.0\").count()\ntotal_population = df.select(\"userid\").distinct().count()\npet_owner_ratio = float(total_pet_owner)/float(total_population)\nprint('total_pet_owner :',total_pet_owner)\nprint('total_population :',total_population)\nprint('pet_owner_ratio :',pet_owner_ratio)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">total_pet_owner : 31524\ntotal_population : 240181\npet_owner_ratio : 0.1312510148596267\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["#### 3. Get insigts of Users"],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.linalg import Vector, Vectors\nfrom pyspark.ml.clustering import LDA\nfrom pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n\npet_owner = prediction.filter(\"prediction = 1.0\").select('userid','vector_stemmed')\n\ncv = CountVectorizer(inputCol=\"vector_stemmed\", outputCol=\"features\",\n                     minTF=2, # minium number of times a word must appear in a document\n                     minDF=4) # minimun number of documents a word must appear in\n\ncountVectorModel = cv.fit(pet_owner)\n\ncountVectors = (countVectorModel\n                .transform(pet_owner)\n                .select(\"userid\", \"features\").cache())\n\nprint(len(countVectorModel.vocabulary))  # how many documents, vocab size\n\nnumTopics = 10 # number of topics\n\nlda = LDA(k = numTopics,\n          maxIter = 50 # number of iterations\n          )\n\nldaModel = lda.fit(countVectors)\n\n\n# Print topics and top-weighted terms\ntopics = ldaModel.describeTopics(maxTermsPerTopic=20)\nvocabArray = countVectorModel.vocabulary\n\nListOfIndexToWords = udf(lambda wl: list([vocabArray[w] for w in wl]))\nFormatNumbers = udf(lambda nl: [\"{:1.4f}\".format(x) for x in nl])\n\ntopics.select(ListOfIndexToWords(topics.termIndices).alias('words')).show(truncate=False, n=numTopics)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">7859\n+----------------------------------------------------------------------------------------------------------------------------------------+\nwords                                                                                                                                   |\n+----------------------------------------------------------------------------------------------------------------------------------------+\n[name, vet, white, eye, boo, half, lion, zoo, sugar, ferret, black, ppl, squirrel, bless, wash, ranch, var, kovu, fluffi, disney]       |\n[dog, puppi, breed, train, kitti, bull, pit, old, poor, collar, fight, see, attack, ear, servic, shelbi, plu, sonic, wolf, guard]       |\n[guy, eat, food, game, snake, leg, call, python, door, sound, egg, coyot, side, ass, meat, ball, box, pig, max, she]                    |\n[love, get, hors, one, like, video, anim, peopl, dont, know, want, time, see, make, realli, say, look, watch, help, day]                |\n[thank, http, got, good, vlog, plz, pitbul, com, youtu, book, american, spider, www, red, youtub, bear, saw, watch, share, ant]         |\n[tree, littl, bee, bir, durian, that, pull, honey, ama, preciou, offic, worth, venom, diy, content, alien, ship, todd, fall, mango]     |\n[cat, kitten, bite, year, adopt, thing, hunt, big, cage, two, rescu, deer, cute, feed, rat, toy, bottl, fox, hedgehog, video]           |\n[que, mucho, para, con, por, una, pero, todo, awesom, como, loki, kss, pelicula, ron, marvel, era, guppi, vengador, howl, eso]          |\n[pleas, bird, fuck, great, human, hand, treat, save, frog, bunni, die, vid, bit, amaz, miss, real, film, feather, like, size]           |\n[like, look, play, stung, robin, tail, anh, danc, chees, mine, birthday, super, wasp, giant, sylvest, taylor, crab, car, send, rottweil]|\n+----------------------------------------------------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["#### 4. Identify Creators With Cat And Dog Owners In The Audience"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import countDistinct\ntmp = prediction.filter(\"prediction = 1.0\")\ntmp.groupBy('creator_name').agg(countDistinct('userid')).sort('count(DISTINCT userid)',ascending= False).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+----------------------+\n        creator_name|count(DISTINCT userid)|\n+--------------------+----------------------+\n            The Dodo|                  2460|\n    Brave Wilderness|                  2283|\n        Robin Seplut|                  1700|\n       Brian Barczyk|                  1162|\nHope For Paws - O...|                  1144|\n  Taylor Nicole Dean|                  1092|\n           Vet Ranch|                   975|\n    Cole &amp; Marmalade|                   840|\n     Gohan The Husky|                   771|\n     Viktor Larkhill|                   745|\nGone to the Snow ...|                   722|\n   Talking Kitty Cat|                   543|\n        Paws Channel|                   491|\n          stacyvlogs|                   481|\n  Think Like A Horse|                   449|\nZak Georges Dog T...|                   391|\n       RaleighLink14|                   364|\n         Info Marvel|                   360|\n            ViralHog|                   294|\n  The Pet Collective|                   284|\n+--------------------+----------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["#### 5. Analysis and Future work"],"metadata":{}},{"cell_type":"code","source":["\nAccording to the work (using only 5% of data because the databricks crashed often with the whole data), around 13% of the total user who commented on Youtube in this dataset are dog or cat owners. The potential topics they are interested in are include vet, white, eye, boo, half, lion, zoo, sugar, ferre, etc. Videos related to these topics could be promoted to these cat or dog owners. Also, 'The Dodo', 'brave wilderness' and 'Robin Seplut' are the top three creators with largest distinct cat or dog owner audience population (based only on the 5% data). Ads targeting cat or dog owners will potentially have the biggest payback cooperating with these creators.\n\nFor future work, this work could be improved in the following aspects: \n  1. Use all of the data.\n  2. Specify the dog and cat owners based on more information.\n"],"metadata":{},"outputs":[],"execution_count":28}],"metadata":{"name":"spark hw2","notebookId":2737912487024782},"nbformat":4,"nbformat_minor":0}
